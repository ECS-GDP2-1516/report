\namedsection{Developing for a Limited Environment}{Shepherd}
% This section talks about the C development

As we have seen in section \ref{ml}, we opted to use a Multilayer Perceptron, which is a good fit for our device as it does not require a great deal of complex mathematical computation. Unfortuneately, however, it does require the use of decimal numbers and uses the sigmoid function, which requires devision and power operations. This section discusses some of the design decisions and sacrifices that were made in order to implement an MLP on the subthreshold Cortex M0+.

\subsection{Sigmoid Function}
The sigmoid function's graph and equation is shown below in figure \ref{fig:sigmoid}. Clearly, this equation causes two issues for the proposed device: firstly, it requires devision, and secondly it requires the use of the constant e, which is a non-integer; using this in an operation involving powers could potentially prove expensive.

The first area of note in this graph is that the value of f(t) quickly starts to tend towards 0 in the negative direction, and 1 in the positive direction. It is not uncommon, therefore, to approximate the value of the function at these extremes. Figure \ref{fig:sigmoid_ends} shows this approximation highlighted in blue for t values outside of the range between 5 and -5; it is plain to see that the difference between figures \ref{fig:sigmoid} and \ref{fig:sigmoid_ends} is difficult to see. On average, we have found that approximately 42\% \todo{This figure is basically made up} of nodes produce values in outside this range meaning they can be approximated to 0 or 1 without the sigmoid function.

\begin{figure}
\centering
\begin{tikzpicture}
\begin{axis}[
    xlabel = $t$,
    ylabel = {$f(t)$},
    legend pos = south east
]
\addplot[color=red, domain=-8:8]{1 / (1 + exp(-x))};
\addlegendentry{$1/(1+exp(-t))$}
\end{axis}
\end{tikzpicture}
\caption{The Sigmoid Function}
\label{fig:sigmoid}
\end{figure}

\begin{figure}
\centering
\begin{tikzpicture}
\begin{axis}[
    xlabel = $t$,
    ylabel = {$f(t)$},
    legend pos = south east
]
\addplot[color=red, domain=-5:5]{1 / (1 + exp(-x))};
\addlegendentry{$1/(1+exp(-t))$}
\addplot[color=blue, domain=5:8]{1};
\addplot[color=blue, domain=-8:-5]{0};
\addlegendentry{$0, 1$}
\end{axis}
\end{tikzpicture}
\caption{The Sigmoid Function with approximated ends}
\label{fig:sigmoid_ends}
\end{figure}

\begin{figure}
\centering
\begin{tikzpicture}
\begin{axis}[
    xlabel = $t$,
    ylabel = {$f(t)$},
    legend pos = south east
]
\addplot[color=red, domain=-8:8, samples=8]{1 / (1 + exp(-x))};
\end{axis}
\end{tikzpicture}
\caption{The Sigmoid Function appoximated with lines}
\label{fig:sigmoid_soft}
\end{figure}

\begin{figure}
\centering
\begin{tikzpicture}
\begin{axis}[
    xlabel = $t$,
    ylabel = {$f(t)$},
    legend pos = south east
]
\addplot[color=red, domain=-5:5]{x / 10 + 1 / 2};
\addlegendentry{$x/10 + 1/2$}
\addplot[color=blue, domain=5:8]{1};
\addplot[color=blue, domain=-8:-5]{0};
\addlegendentry{$0, 1$}
\end{axis}
\end{tikzpicture}
\caption{The Sigmoid Function approximated as a single line}
\label{fig:sigmoid_hard}
\end{figure}

For the remaining range, between 5 and -5, the sigmoid function is does not tend to any fixed value, so a different approximation must be used. Fortuneately, the sigmoid's ``S''-like shape lends itself to being easily split into a series of smaller lines, as shown in figure \ref{fig:sigmoid_soft}. This provides a suitable level of accuracy with far lower computing overhead. However, as beneficial as the approximation in figure \ref{fig:sigmoid_soft} is, it is possible to approximate this further. We have found that approximating the sigmoid function with a single
linear equation has proved satisfactory and has only lead to an observed drop in accuracy by 1\% \todo{Again, totally made up}. This is shown in this section's final figure: \ref{fig:sigmoid_hard}.

\subsection{Floating point}
The lack of floating point support in the proposed device provided the team with a large challenge, as the neural network is entirely based upon a series of non-integer input weights, and the ability for each node to produce non integer output values. In order to get around this problem, the team opted for a fixed-point implementation: the weights of each node are multiplied by a scale factor and the resulting integer part is taken as the value.

This form is convienient as it makes the multiplication and addition of such scaled values straight forward: for addition of values using the same scale factor, it is sufficient to simply add the integers as though they were normal. The equation below illustrates this:

$$x*S+y*S=(x+y)*S$$

For the case of multiplation, the scale factors of each number must be added, meaning that when two numbers with a scale factor of $S$ are multiplied, the resulting number's scale factor is $S^2$:

$$(x*S)(y*S)=(x*y)*S^2$$

This can be easily rectified by simplying dividing the result by $S$ again to return to the original scale factor:

$$\frac{(x*y)*S^2}{S}=(x*y)*S$$

As division is not supported on our system, we have defined our scale factor $S$ as a power of 2: $S=2^B$. We are then able to approximate divisions of powers of 2, by simply performing a bitshift:

$$X/2^B\approx X \gg B$$

\note{No division - have to scale ints or use floating point lib?}

\note{Hilarious lack of space - recompile stuff}

\note{Java Harness?}