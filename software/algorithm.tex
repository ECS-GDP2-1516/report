\namedsection{Developing the Algorithm}{Shepherd}

Taking into account the constraints of the system, it was decided to focus on the Multilayer Perceptron as it does not require matrix maths and typically has a smaller network of nodes. This section discusses the process by which we developed an implementation of this algorithm.

\subsection{Existing Weka Implementation \label{sec:weka-code}}

As Weka had been used to determine and train the best algorithm, it was decided to inspect its source code to obtain a starting point for the algorithm development. This, while providing a good insight into the manner in which the algorithm functions, highlighted some key areas of inefficiencies that would have to be avoided when working on a sub-threshold device.

\subsubsection{Backward Propagation}

Weka's implementation of the Multilayer Perception uses ``backward propagation'', which recursively works backwards in a depth-first manner from each output node towards the input layer. The function which calculates a node's output does so by taking the summation of each of its input values multiplied by their respective input weights. Each input value, however, is requested on the fly from the inputting node by calculating its output which in turn causes it to perform the same loop over its input nodes.

\begin{lstlisting}[language=Java,caption={Weka's method of calculating a node's output}]
class Node
{
    double weights[NUM_INPUTS];
    Node inputs[NUM_INPUTS];
    double baseValue;

    double output()
    {
        double out = base;

        for (int i = 0; i < NUM_INPUTS; i++)
        {
            out += weights[i] * inputs[i].output();
        }

        return out;
    }
}
\end{lstlisting}

A side effect of this is that all nodes, other than the output nodes are visited multiple times. For example, for the network below, $o_1.output()$ will loop over $m_1$, $m_2$ and $m_3$, calling \verb|output()| on each, each of which will loop over $i_1$, $i_2$ and $i_3$, meaning the input nodes will be visited a total of 3 times each. This process must then be repeated for $o_2$, causing the middle layer nodes to each be revisited, and the input layers revisited another 3 times, and again for $o_3$. In total each input node is accessed 9 times, the middle layer 3 times each.

\begin{figure}
    \centering
    \includegraphics{eg-ml}
    \caption{Example Multilayer Perception Network}
    \label{fig:eg-ml}
\end{figure}

Weka reduces this by storing a node's value after an \verb|output()| call. When \verb|output()| is called on a node again, it checks if it has already been called and returns the cached value if it has, as shown in listing \ref{lst:weka-cached}.

\clearpage
\begin{lstlisting}[language=Java,caption={Weka's method of preventing repeated calculations},label={lst:weka-cached}]
class Node
{
    boolean visited = false;
    double value;

    double output()
    {
        if (this.visited) return this.value;

        // Otherwise perform recursive calculation above

        this.visited = true;
        return this.value = out;
    }
}
\end{lstlisting}

While this does succeed in reducing the number of unnecessary node visits, it does not eliminate them; each node in the network, other than the output nodes, are visited three times each with this implementation. This scheme also adds two new node properties which must be stored and manipulated; the side effect of this is an increase in instructions and memory requirement, which is not ideal for sub-threshold development.

The algorithm itself still relies on recursion - this is also suboptimal as this requires extensive use of the stack. This should be avoided due to the fact the memory available on the system is very limited.

\subsubsection{The Alternative: Forward Propagation}

As a result of the inerrant inefficiencies in a Backward Propagation implementation, it was decided instead to redesign the algorithm, employing Forwards Propagation. This technique, in contrast to the one described above, starts from the input layer nodes and works forward in a breadth first manner.

For the example in figure \ref{fig:eg-ml}, this means the algorithm would first write the outputs of $i_1$, $i_2$ and $i_3$ into three memory spaces. Following this, three new memory spaces are reserved, in which the base values of $m_1$, $m_2$ and $m_3$ are placed. The algorithm then loops over the input nodes and, for each one, adds the multiplication of its value by its parent weight to the parent's memory space. Once complete, the outputs for all three middle layer nodes have been completed so the process can be shifted one layer to the right and repeated. An added benefit here is that the input layers are now no longer required, so their memory can be freed and reused if desired.

\clearpage

This method requires no stack usage and can operate in a small memory space, shown in equation \ref{eq:algo-size}. For our algorithm, which has the same number of layers as the example above, this results in only 5 32bit memory spaces: 20 bytes.

\begin{equation}
\label{eq:algo-size}
\max\{\forall L_n \in LAYERS \wedge L_{n+1} \in LAYERS : |L_n|+|L_{n+1}|\}
\end{equation}

\subsection{Moving to C Code}

During development, the team designed the C implementation to work on a desktop machine. This allowed work to focus initially on the construction of an algorithm which matched the accuracy given by Weka, without being too distracted by the finer details of embedded development, and with convenient access to debugging and \verb|stdio| tools. 

Weka is a very useful tool for generating and training machine learning algorithms. Unfortunately, it only allows exporting to \verb|.model| files, which are simply Java serialized objects. As such, Weka's source code had to be modified in order to obtain the number, structure and weightings of the nodes in the MLP's neural network.

Once the algorithm was designed, it was decided to modify and improve the Java code such that it became a harness for our C code, processing the nodes prior to output, removing unused layers and returning the required data in generated C code. This proved very useful, as training the networks and trying various settings took time and was repeated many times during development. Using this harness allowed the team to quickly deploy updated models onto the device without the need to reprogram the C code for each test.