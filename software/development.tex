\namedsection{Developing for a Limited Environment}{Shepherd}
% This section talks about the C development

The Multilayer Perceptron is a good fit for a subthreshold Cortex M0+ as it does not require a great deal of complex mathematical computation. Unfortuneately, however, it does require the use of decimal numbers and uses the sigmoid function, which requires division and power operations. This section discusses some of the design decisions and sacrifices that were made in order to implement an MLP on such a constrained system.

\subsection{Sigmoid Function}
The sigmoid function is applied to each node's output, after the summation of its weighted inputs is calculated, as discussed above in section \ref{sec:weka-code}; its graph and equation is shown below in figure \ref{fig:sigmoid}. Clearly, this equation causes two issues for the proposed device: firstly, it requires division, and secondly it requires the use of the constant \verb|e|, which is a non-integer; using this in an operation involving powers could potentially prove expensive.

The first area of note in this graph is that the value of $f(t)$ quickly starts to tend towards 0 in the negative direction, and 1 in the positive direction. It is not uncommon, therefore, to approximate the value of the function at these extremes \cite{sigmoid_approx}. Figure \ref{fig:sigmoid-ends} shows this approximation highlighted in bold for \verb|t| values outside of the range between 5 and -5; it is plain to see that the error introduced here is minimal.

\begin{figure}[!h]
    \centering
    \subfigure[Normal Sigmoid Function]{\label{fig:sigmoid}\includegraphics[width=70mm]{figures/sigmoid.pdf}}
    \subfigure[Approximated Ends]{\label{fig:sigmoid-ends}\includegraphics[width=70mm]{figures/sigmoid-ends.pdf}}
    \subfigure[Approximated With Lines]{\label{fig:sigmoid-soft}\includegraphics[width=70mm]{figures/sigmoid-soft.pdf}}
    \subfigure[Approximated With Linear Equation]{\label{fig:sigmoid-hard}\includegraphics[width=70mm]{figures/sigmoid-hard.pdf}}
    \caption{Sigmoid Functions \label{fig:sigmoid-options}}
\end{figure}

For the remaining range, between 5 and -5, the sigmoid function does not tend to any fixed value, so a different approximation must be used. Fortunately, the sigmoid's ``S''-like shape lends itself to being easily split into a series of smaller lines, as shown in figure \ref{fig:sigmoid-soft}. This provides a suitable level of accuracy with far lower computing overhead as each line can be defined using only a linear equation. However, as beneficial as the approximation in figure \ref{fig:sigmoid-soft} is, it is possible to approximate this further: using a single linear equation. This is shown in this section's final figure: \ref{fig:sigmoid-hard}. When run on the training dataset, which under non-constrained conditions achieves 94.5\% accuracy, this approximation only yields an observed drop of 3\%. This margin of error is acceptable as it can be dealt with by the added heuristics, as discussed in section \ref{sec:heuristics-alternative} above.

\subsection{Floating point \label{sec:floating-point}}
The lack of floating point support in the proposed device created a large challenge, as the neural network is entirely based upon a series of non-integer input weights, and the ability for each node to produce non integer output values. As mentioned in section \ref{sec:cortex-limitations}, the device does not have native support for hardware floating point, and floating point libraries give too much overhead to be feasible for this project. In order to get around this problem, the team opted for a fixed-point implementation: the weights of each node are multiplied by a scale factor and the resulting integer part is taken as the value.

This form is convenient as it makes the multiplication and addition of such scaled values straightforward: for addition of values using the same scale factor, it is sufficient to simply add the integers as though they were normal. The equation below illustrates this:

\begin{equation}
\label{eq:bits:addition}
x*S+y*S=(x+y)*S
\end{equation}

For the case of multiplication, the scale factors of each number must also be multiplied, meaning that when two numbers with a scale factor of $S$ are multiplied, the resulting number's scale factor is $S^2$:

\begin{equation}
\label{eq:bits:multiplication}
(x*S)(y*S)=(x*y)*S^2
\end{equation}

This can be easily rectified by simplifying dividing the result by $S$ again to return to the original scale factor:

\begin{equation}
\label{eq:bits:rescale}
\frac{(x*y)*S^2}{S}=(x*y)*S
\end{equation}

As division is not supported on our system, we have defined our scale factor $S$ as a power of 2: $S=2^B$. We are then able to approximate divisions of powers of 2, by simply performing a bit shift:

\begin{equation}
\label{eq:bits:div_approx}
X/2^B\approx X \gg B
\end{equation}

\subsubsection{Using this on the Device}

To decide the bit scale factor, $B$, for the exercise detection algorithm, it is important to consider the memory restrictions on variable size. At a hardware level, 32bits, or 4 bytes, is the maximum size an integer can be. As the algorithm requires the multiplication of numbers, ideally these would need to fit into just a 2 bytes space, as the multiplication of numbers results in the addition of powers, as shown in equation \ref{eq:bits:multiplication-shift}.

\begin{equation}
\label{eq:bits:multiplication-shift}
(x*2^B)(y*2^B)=(x*y)*(2^B)^2=(x*y)*2^{2B}
\end{equation}

As such, the value of $B$ has to be high enough such that precision is not lost unnecessarily, but low enough such that $|x*2^B|<2^{16}$. To solve this, the highest possible floating point value must be known; in the case of the algorithm, this is the base value of the first node in the middle layer: -10.6 (3s.f.); this was obtained from the training, as discussed in section \ref{ml}. Using this value in the rearranged equation, \ref{eq:bits:number-calc}, gives a bit shift of 12.

\begin{equation}
\label{eq:bits:number-calc}
B<\lfloor\log_{2}\left\{\frac{2^{16}}{|x|}\right\}\rfloor
\end{equation}
